{
  "Vaswani2017AttentionIA": {
    "title": "Attention is All you Need",
    "bibtex": "@Article{Vaswani2017AttentionIA,\n author = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and I. Polosukhin},\n booktitle = {Neural Information Processing Systems},\n pages = {5998-6008},\n title = {Attention is All you Need},\n year = {2017}\n}\n",
    "abstract": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
    "authors": [
      "Ashish Vaswani",
      "Noam Shazeer",
      "Niki Parmar",
      "Jakob Uszkoreit",
      "Llion Jones",
      "Aidan N. Gomez",
      "Lukasz Kaiser",
      "I. Polosukhin"
    ],
    "venue": "Neural Information Processing Systems",
    "year": 2017,
    "url": "https://www.semanticscholar.org/paper/204e3073870fae3d05bcbc2f6a8e263d9b72e776"
  },
  "Touvron2020TrainingDI": {
    "title": "Training data-efficient image transformers & distillation through attention",
    "bibtex": "@Article{Touvron2020TrainingDI,\n author = {Hugo Touvron and M. Cord and Matthijs Douze and Francisco Massa and Alexandre Sablayrolles and Herv'e J'egou},\n booktitle = {International Conference on Machine Learning},\n pages = {10347-10357},\n title = {Training data-efficient image transformers & distillation through attention},\n year = {2020}\n}\n",
    "abstract": "Recently, neural networks purely based on attention were shown to address image understanding tasks such as image classification. However, these visual transformers are pre-trained with hundreds of millions of images using an expensive infrastructure, thereby limiting their adoption. In this work, we produce a competitive convolution-free transformer by training on Imagenet only. We train them on a single computer in less than 3 days. Our reference vision transformer (86M parameters) achieves top-1 accuracy of 83.1% (single-crop evaluation) on ImageNet with no external data. More importantly, we introduce a teacher-student strategy specific to transformers. It relies on a distillation token ensuring that the student learns from the teacher through attention. We show the interest of this token-based distillation, especially when using a convnet as a teacher. This leads us to report results competitive with convnets for both Imagenet (where we obtain up to 85.2% accuracy) and when transferring to other tasks. We share our code and models.",
    "authors": [
      "Hugo Touvron",
      "M. Cord",
      "Matthijs Douze",
      "Francisco Massa",
      "Alexandre Sablayrolles",
      "Herv'e J'egou"
    ],
    "venue": "International Conference on Machine Learning",
    "year": 2020,
    "url": "https://www.semanticscholar.org/paper/ad7ddcc14984caae308c397f1a589aae75d4ab71"
  },
  "Woo2018CBAMCB": {
    "title": "CBAM: Convolutional Block Attention Module",
    "bibtex": "@Article{Woo2018CBAMCB,\n author = {Sanghyun Woo and Jongchan Park and Joon-Young Lee and In-So Kweon},\n booktitle = {European Conference on Computer Vision},\n journal = {ArXiv},\n title = {CBAM: Convolutional Block Attention Module},\n volume = {abs/1807.06521},\n year = {2018}\n}\n",
    "abstract": "We propose Convolutional Block Attention Module (CBAM), a simple yet effective attention module for feed-forward convolutional neural networks. Given an intermediate feature map, our module sequentially infers attention maps along two separate dimensions, channel and spatial, then the attention maps are multiplied to the input feature map for adaptive feature refinement. Because CBAM is a lightweight and general module, it can be integrated into any CNN architectures seamlessly with negligible overheads and is end-to-end trainable along with base CNNs. We validate our CBAM through extensive experiments on ImageNet-1K, MS COCO detection, and VOC 2007 detection datasets. Our experiments show consistent improvements in classification and detection performances with various models, demonstrating the wide applicability of CBAM. The code and models will be publicly available.",
    "authors": [
      "Sanghyun Woo",
      "Jongchan Park",
      "Joon-Young Lee",
      "In-So Kweon"
    ],
    "venue": "European Conference on Computer Vision",
    "year": 2018,
    "url": "https://www.semanticscholar.org/paper/de95601d9e3b20ec51aa33e1f27b1880d2c44ef2"
  },
  "Subakan2020AttentionIA": {
    "title": "Attention Is All You Need In Speech Separation",
    "bibtex": "@Article{Subakan2020AttentionIA,\n author = {Cem Subakan and M. Ravanelli and Samuele Cornell and Mirko Bronzi and Jianyuan Zhong},\n booktitle = {IEEE International Conference on Acoustics, Speech, and Signal Processing},\n journal = {ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},\n pages = {21-25},\n title = {Attention Is All You Need In Speech Separation},\n year = {2020}\n}\n",
    "abstract": "Recurrent Neural Networks (RNNs) have long been the dominant architecture in sequence-to-sequence learning. RNNs, however, are inherently sequential models that do not allow parallelization of their computations. Transformers are emerging as a natural alternative to standard RNNs, replacing recurrent computations with a multi-head attention mechanism.In this paper, we propose the SepFormer, a novel RNN-free Transformer-based neural network for speech separation. The Sep-Former learns short and long-term dependencies with a multi-scale approach that employs transformers. The proposed model achieves state-of-the-art (SOTA) performance on the standard WSJ0-2/3mix datasets. It reaches an SI-SNRi of 22.3 dB on WSJ0-2mix and an SI-SNRi of 19.5 dB on WSJ0-3mix. The SepFormer inherits the parallelization advantages of Transformers and achieves a competitive performance even when downsampling the encoded representation by a factor of 8. It is thus significantly faster and it is less memory-demanding than the latest speech separation systems with comparable performance.",
    "authors": [
      "Cem Subakan",
      "M. Ravanelli",
      "Samuele Cornell",
      "Mirko Bronzi",
      "Jianyuan Zhong"
    ],
    "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
    "year": 2020,
    "url": "https://www.semanticscholar.org/paper/51c9d4d2f50ac5707c1f889aa97f08350d549132"
  },
  "Choi2020ChannelAI": {
    "title": "Channel Attention Is All You Need for Video Frame Interpolation",
    "bibtex": "@Article{Choi2020ChannelAI,\n author = {Myungsub Choi and Heewon Kim and Bohyung Han and N. Xu and Kyoung Mu Lee},\n booktitle = {AAAI Conference on Artificial Intelligence},\n pages = {10663-10671},\n title = {Channel Attention Is All You Need for Video Frame Interpolation},\n year = {2020}\n}\n",
    "abstract": "Prevailing video frame interpolation techniques rely heavily on optical flow estimation and require additional model complexity and computational cost; it is also susceptible to error propagation in challenging scenarios with large motion and heavy occlusion. To alleviate the limitation, we propose a simple but effective deep neural network for video frame interpolation, which is end-to-end trainable and is free from a motion estimation network component. Our algorithm employs a special feature reshaping operation, referred to as PixelShuffle, with a channel attention, which replaces the optical flow computation module. The main idea behind the design is to distribute the information in a feature map into multiple channels and extract motion information by attending the channels for pixel-level frame synthesis. The model given by this principle turns out to be effective in the presence of challenging motion and occlusion. We construct a comprehensive evaluation benchmark and demonstrate that the proposed approach achieves outstanding performance compared to the existing models with a component for optical flow computation.",
    "authors": [
      "Myungsub Choi",
      "Heewon Kim",
      "Bohyung Han",
      "N. Xu",
      "Kyoung Mu Lee"
    ],
    "venue": "AAAI Conference on Artificial Intelligence",
    "year": 2020,
    "url": "https://www.semanticscholar.org/paper/bb4a9650ca3946c70a7e92007cc61dc0dfd75522"
  }
}